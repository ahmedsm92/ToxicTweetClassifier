{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1509808",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd\n",
    "\n",
    "import re\n",
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "from string import ascii_lowercase\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "import itertools\n",
    "import io\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "from functools import reduce\n",
    "from tensorflow import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "from keras.layers import BatchNormalization\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "import talos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5e79f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train=pd.read_csv('train.csv')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab0ec089",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a20d8392",
   "metadata": {},
   "outputs": [],
   "source": [
    "test=pd.read_csv('test.csv')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7251c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data exploration stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3536101a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id               False\n",
       "comment_text     False\n",
       "toxic            False\n",
       "severe_toxic     False\n",
       "obscene          False\n",
       "threat           False\n",
       "insult           False\n",
       "identity_hate    False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42cc5e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "y = train[labels].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9c2d172",
   "metadata": {},
   "outputs": [],
   "source": [
    "RE_PATTERNS = {\n",
    "    ' american ':\n",
    "        [\n",
    "            'amerikan'\n",
    "        ],\n",
    "\n",
    "    ' adolf ':\n",
    "        [\n",
    "            'adolf'\n",
    "        ],\n",
    "\n",
    "\n",
    "    ' hitler ':\n",
    "        [\n",
    "            'hitler'\n",
    "        ],\n",
    "\n",
    "    ' fuck':\n",
    "        [\n",
    "            '(f)(u|[^a-z0-9 ])(c|[^a-z0-9 ])(k|[^a-z0-9 ])([^ ])*',\n",
    "            '(f)([^a-z]*)(u)([^a-z]*)(c)([^a-z]*)(k)',\n",
    "            ' f[!@#\\$%\\^\\&\\*]*u[!@#\\$%\\^&\\*]*k', 'f u u c',\n",
    "            '(f)(c|[^a-z ])(u|[^a-z ])(k)', r'f\\*',\n",
    "            'feck ', ' fux ', 'f\\*\\*', 'f**k','fu*k',\n",
    "            'f\\-ing', 'f\\.u\\.', 'f###', ' fu ', 'f@ck', 'f u c k', 'f uck', 'f ck'\n",
    "        ],\n",
    "\n",
    "    ' ass ':\n",
    "        [\n",
    "            '[^a-z]ass ', '[^a-z]azz ', 'arrse', ' arse ', '@\\$\\$',\n",
    "            '[^a-z]anus', ' a\\*s\\*s', '[^a-z]ass[^a-z ]',\n",
    "            'a[@#\\$%\\^&\\*][@#\\$%\\^&\\*]', '[^a-z]anal ', 'a s s','a55', '@$$'\n",
    "        ],\n",
    "\n",
    "    ' ass hole ':\n",
    "        [\n",
    "            ' a[s|z]*wipe', 'a[s|z]*[w]*h[o|0]+[l]*e', '@\\$\\$hole', 'a**hole'\n",
    "        ],\n",
    "\n",
    "    ' bitch ':\n",
    "        [\n",
    "            'b[w]*i[t]*ch', 'b!tch',\n",
    "            'bi\\+ch', 'b!\\+ch', '(b)([^a-z]*)(i)([^a-z]*)(t)([^a-z]*)(c)([^a-z]*)(h)',\n",
    "            'biatch', 'bi\\*\\*h', 'bytch', 'b i t c h', 'b!tch', 'bi+ch', 'l3itch'\n",
    "        ],\n",
    "\n",
    "    ' bastard ':\n",
    "        [\n",
    "            'ba[s|z]+t[e|a]+rd'\n",
    "        ],\n",
    "\n",
    "    ' trans gender':\n",
    "        [\n",
    "            'transgender'\n",
    "        ],\n",
    "\n",
    "    ' gay ':\n",
    "        [\n",
    "            'gay'\n",
    "        ],\n",
    "\n",
    "    ' cock ':\n",
    "        [\n",
    "            '[^a-z]cock', 'c0ck', '[^a-z]cok ', 'c0k', '[^a-z]cok[^aeiou]', ' cawk',\n",
    "            '(c)([^a-z ])(o)([^a-z ]*)(c)([^a-z ]*)(k)', 'c o c k'\n",
    "        ],\n",
    "\n",
    "    ' dick ':\n",
    "        [\n",
    "            ' dick[^aeiou]', 'deek', 'd i c k', 'dik'\n",
    "        ],\n",
    "\n",
    "    ' suck ':\n",
    "        [\n",
    "            'sucker', '(s)([^a-z ]*)(u)([^a-z ]*)(c)([^a-z ]*)(k)', 'sucks', '5uck', 's u c k'\n",
    "        ],\n",
    "\n",
    "    ' cunt ':\n",
    "        [\n",
    "            'cunt', 'c u n t'\n",
    "        ],\n",
    "\n",
    "    ' bull shit ':\n",
    "        [\n",
    "            'bullsh\\*t', 'bull\\$hit'\n",
    "        ],\n",
    "\n",
    "    ' homo sex ual':\n",
    "        [\n",
    "            'homosexual'\n",
    "        ],\n",
    "\n",
    "    ' jerk ':\n",
    "        [\n",
    "            'jerk'\n",
    "        ],\n",
    "\n",
    "    ' idiot ':\n",
    "        [\n",
    "            'i[d]+io[t]+', '(i)([^a-z ]*)(d)([^a-z ]*)(i)([^a-z ]*)(o)([^a-z ]*)(t)', 'idiots'\n",
    "                                                                                      'i d i o t'\n",
    "        ],\n",
    "\n",
    "    ' dumb ':\n",
    "        [\n",
    "            '(d)([^a-z ]*)(u)([^a-z ]*)(m)([^a-z ]*)(b)'\n",
    "        ],\n",
    "\n",
    "    ' shit ':\n",
    "        [\n",
    "            'shitty', '(s)([^a-z ]*)(h)([^a-z ]*)(i)([^a-z ]*)(t)', 'shite', '\\$hit', 's h i t', '$h1t'\n",
    "        ],\n",
    "\n",
    "    ' shit hole ':\n",
    "        [\n",
    "            'shythole'\n",
    "        ],\n",
    "\n",
    "    ' retard ':\n",
    "        [\n",
    "            'returd', 'retad', 'retard', 'wiktard', 'wikitud'\n",
    "        ],\n",
    "\n",
    "    ' rape ':\n",
    "        [\n",
    "            ' raped'\n",
    "        ],\n",
    "\n",
    "    ' dumb ass':\n",
    "        [\n",
    "            'dumbass', 'dubass'\n",
    "        ],\n",
    "\n",
    "    ' ass head':\n",
    "        [\n",
    "            'butthead'\n",
    "        ],\n",
    "\n",
    "    ' sex ':\n",
    "        [\n",
    "            'sexy', 's3x', 'sexuality'\n",
    "        ],\n",
    "\n",
    "\n",
    "    ' nigger ':\n",
    "        [\n",
    "            'nigger', 'ni[g]+a', ' nigr ', 'negrito', 'niguh', 'n3gr', 'n i g g e r'\n",
    "        ],\n",
    "\n",
    "    ' shut the fuck up':\n",
    "        [\n",
    "            'stfu', 'st*u'\n",
    "        ],\n",
    "\n",
    "    ' pussy ':\n",
    "        [\n",
    "            'pussy[^c]', 'pusy', 'pussi[^l]', 'pusses', 'p*ssy'\n",
    "        ],\n",
    "\n",
    "    ' faggot ':\n",
    "        [\n",
    "            'faggot', ' fa[g]+[s]*[^a-z ]', 'fagot', 'f a g g o t', 'faggit',\n",
    "            '(f)([^a-z ]*)(a)([^a-z ]*)([g]+)([^a-z ]*)(o)([^a-z ]*)(t)', 'fau[g]+ot', 'fae[g]+ot',\n",
    "        ],\n",
    "\n",
    "    ' mother fucker':\n",
    "        [\n",
    "            ' motha ', ' motha f', ' mother f', 'motherucker',\n",
    "        ],\n",
    "\n",
    "    ' whore ':\n",
    "        [\n",
    "            'wh\\*\\*\\*', 'w h o r e'\n",
    "        ],\n",
    "    ' fucking ':\n",
    "        [\n",
    "            'f*$%-ing'\n",
    "        ],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3352cef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text,remove_repeat_text=True, remove_patterns_text=True, is_lower=True):\n",
    "\n",
    "  if is_lower:\n",
    "    text=text.lower()\n",
    "    \n",
    "  if remove_patterns_text:\n",
    "    for target, patterns in RE_PATTERNS.items():\n",
    "      for pat in patterns:\n",
    "        text=str(text).replace(pat, target)\n",
    "\n",
    "  if remove_repeat_text:\n",
    "    text = re.sub(r'(.)\\1{2,}', r'\\1', text) \n",
    "\n",
    "  text = str(text).replace(\"\\n\", \" \")\n",
    "  text = re.sub(r'[^\\w\\s]',' ',text)\n",
    "  text = re.sub('[0-9]',\"\",text)\n",
    "  text = re.sub(\" +\", \" \", text)\n",
    "  text = re.sub(\"([^\\x00-\\x7F])+\",\" \",text)\n",
    "  return text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e3cd42e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train['comment_text']=train['comment_text'].apply(lambda x: clean_text(x))\n",
    "test['comment_text']=test['comment_text'].apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "67231d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_train=train['comment_text']\n",
    "comments_test=test['comment_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8b57a7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_train=list(comments_train)\n",
    "comments_test=list(comments_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "55e909ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4b0f09f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemma(text, lemmatization=True):\n",
    "  output=\"\"\n",
    "  if lemmatization:\n",
    "    text=text.split(\" \")\n",
    "    for word in text:\n",
    "       word1 = wordnet_lemmatizer.lemmatize(word, pos = \"n\")\n",
    "       word2 = wordnet_lemmatizer.lemmatize(word1, pos = \"v\")\n",
    "       word3 = wordnet_lemmatizer.lemmatize(word2, pos = \"a\")\n",
    "       word4 = wordnet_lemmatizer.lemmatize(word3, pos = \"r\")\n",
    "       output=output + \" \" + word4\n",
    "  else:\n",
    "    output=text\n",
    "  \n",
    "  return str(output.strip()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d7048e53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c067d535efad4187bfe0e1854e7a7994",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/159571 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ipywidgets import FloatProgress\n",
    "\n",
    "lemmatized_train_data = [] \n",
    "\n",
    "for line in tqdm_notebook(comments_train, total=159571): \n",
    "    lemmatized_train_data.append(lemma(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3e95f45e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52868f41d326414c9f422924706355ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/153164 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lemmatized_test_data = [] \n",
    "\n",
    "for line in tqdm_notebook(comments_test, total=len(comments_test)): \n",
    "    lemmatized_test_data.append(lemma(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cf9013bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_list=STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c9a8115b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iter_all_strings():\n",
    "    for size in itertools.count(1):\n",
    "        for s in itertools.product(ascii_lowercase, repeat=size):\n",
    "            yield \"\".join(s)\n",
    "\n",
    "dual_alpha_list=[]\n",
    "for s in iter_all_strings():\n",
    "    dual_alpha_list.append(s)\n",
    "    if s == 'zz':\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c7c28aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "dual_alpha_list.remove('i')\n",
    "dual_alpha_list.remove('a')\n",
    "dual_alpha_list.remove('am')\n",
    "dual_alpha_list.remove('an')\n",
    "dual_alpha_list.remove('as')\n",
    "dual_alpha_list.remove('at')\n",
    "dual_alpha_list.remove('be')\n",
    "dual_alpha_list.remove('by')\n",
    "dual_alpha_list.remove('do')\n",
    "dual_alpha_list.remove('go')\n",
    "dual_alpha_list.remove('he')\n",
    "dual_alpha_list.remove('hi')\n",
    "dual_alpha_list.remove('if')\n",
    "dual_alpha_list.remove('is')\n",
    "dual_alpha_list.remove('in')\n",
    "dual_alpha_list.remove('me')\n",
    "dual_alpha_list.remove('my')\n",
    "dual_alpha_list.remove('no')\n",
    "dual_alpha_list.remove('of')\n",
    "dual_alpha_list.remove('on')\n",
    "dual_alpha_list.remove('or')\n",
    "dual_alpha_list.remove('ok')\n",
    "dual_alpha_list.remove('so')\n",
    "dual_alpha_list.remove('to')\n",
    "dual_alpha_list.remove('up')\n",
    "dual_alpha_list.remove('us')\n",
    "dual_alpha_list.remove('we')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7f61e990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!!\n"
     ]
    }
   ],
   "source": [
    "for letter in dual_alpha_list:\n",
    "    stopword_list.add(letter)\n",
    "print(\"Done!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "52a9d753",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_stopwords(data, search_stop=True):\n",
    "  output=\"\"\n",
    "  if search_stop:\n",
    "    data=data.split(\" \")\n",
    "    for word in data:\n",
    "      if not word in stopword_list:\n",
    "        output=output+\" \"+word \n",
    "  else:\n",
    "    output=data\n",
    "\n",
    "  return str(output.strip())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ddecc3fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b25a10c6dae0439f95e9c8c0cd208405",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/159571 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "potential_stopwords = [] \n",
    "\n",
    "for line in tqdm_notebook(lemmatized_train_data, total=159571): \n",
    "    potential_stopwords.append(search_stopwords(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bd8099fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_combine_a(stopword):\n",
    "  final_a=\"\"\n",
    "  for item in range(39893):\n",
    "    final_a=final_a+\" \"+stopword[item]\n",
    "  return final_a\n",
    "\n",
    "def string_combine_b(stopword):\n",
    "  final_b=\"\"\n",
    "  for item in range(39893,79785):\n",
    "    final_b=final_b+\" \"+stopword[item]\n",
    "  return final_b\n",
    "\n",
    "def string_combine_c(stopword):\n",
    "  final_c=\"\"\n",
    "  for item in range(79785,119678):\n",
    "    final_c=final_c+\" \"+stopword[item]\n",
    "  return final_c\n",
    "\n",
    "def string_combine_d(stopword):\n",
    "  final_d=\"\"\n",
    "  for item in range(119678,159571):\n",
    "    final_d=final_d+\" \"+stopword[item]\n",
    "    \n",
    "  return final_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d7f80197",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_string_potential_a=string_combine_a(potential_stopwords)\n",
    "total_string_potential_b=string_combine_b(potential_stopwords)\n",
    "total_string_potential_c=string_combine_c(potential_stopwords)\n",
    "total_string_potential_d=string_combine_d(potential_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "01d8901b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_count(str):\n",
    "    counts = dict()\n",
    "    words = str.split()\n",
    "\n",
    "    for word in words:\n",
    "        if word in counts:\n",
    "            counts[word] += 1\n",
    "        else:\n",
    "            counts[word] = 1\n",
    "\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "64e1508b",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_string_potential_a_dict=word_count(total_string_potential_a)\n",
    "total_string_potential_b_dict=word_count(total_string_potential_b)\n",
    "total_string_potential_c_dict=word_count(total_string_potential_c)\n",
    "total_string_potential_d_dict=word_count(total_string_potential_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9c76016e",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_string_potential_a_df = pd.DataFrame(list(total_string_potential_a_dict.items()),columns = ['Word','Count'])\n",
    "total_string_potential_b_df = pd.DataFrame(list(total_string_potential_b_dict.items()),columns = ['Word','Count'])\n",
    "total_string_potential_c_df = pd.DataFrame(list(total_string_potential_c_dict.items()),columns = ['Word','Count'])\n",
    "total_string_potential_d_df = pd.DataFrame(list(total_string_potential_d_dict.items()),columns = ['Word','Count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "03b9ee98",
   "metadata": {},
   "outputs": [],
   "source": [
    "top50_potential_stopwords_a=total_string_potential_a_df.sort_values(by=['Count'],ascending=False).head(50)\n",
    "top50_potential_stopwords_b=total_string_potential_b_df.sort_values(by=['Count'],ascending=False).head(50)\n",
    "top50_potential_stopwords_c=total_string_potential_c_df.sort_values(by=['Count'],ascending=False).head(50)\n",
    "top50_potential_stopwords_d=total_string_potential_d_df.sort_values(by=['Count'],ascending=False).head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "48d2590f",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_potential_stopwords=list(reduce(set.intersection,map(set,[top50_potential_stopwords_a.Word,top50_potential_stopwords_b.Word,top50_potential_stopwords_c.Word,top50_potential_stopwords_d.Word])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "42956eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['help', 'link', 'want', 'editor', 'information', 'add', 'deletion', 'think', 'wikipedia', 'change', 'talk', 'source', 'edit', 'read', 'fuck', 'section', 'mean', 'write', 'page', 'point', 'need', 'thank', 'doe', 'thing', 'time', 'user', 'way', 'image', 'new', 'block', 'good', 'use', 'delete', 'list', 'question', 'work', 'remove', 'don', 'find', 'look', 'comment', 'like', 'fact', 'article', 'people', 'try', 'reference', 'know']\n"
     ]
    }
   ],
   "source": [
    "print(common_potential_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6193b8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "potential_stopwords=['editor', 'reference', 'thank', 'work','find', 'good', 'know', 'like', 'look', 'thing', 'want', 'time', 'list', 'section','wikipedia', 'doe', 'add','new', 'try', 'think', 'write','use', 'user', 'way', 'page']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6c0b2a07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!!\n"
     ]
    }
   ],
   "source": [
    "for word in potential_stopwords:\n",
    "    stopword_list.add(word)\n",
    "print(\"Done!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4002de5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text, remove_stop=True):\n",
    "  output = \"\"\n",
    "  if remove_stop:\n",
    "    text=text.split(\" \")\n",
    "    for word in text:\n",
    "      if word not in stopword_list:\n",
    "        output=output + \" \" + word\n",
    "  else :\n",
    "    output=text\n",
    "\n",
    "  return str(output.strip())      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1ddd9297",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cc80fd125f444cbbc3637d62879fe03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/159571 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "processed_train_data = [] \n",
    "\n",
    "for line in tqdm_notebook(lemmatized_train_data, total=159571): \n",
    "    processed_train_data.append(remove_stopwords(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "78f26a16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3622d83df0824797bf8951ba2570b021",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/153164 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "processed_test_data = [] \n",
    "\n",
    "for line in tqdm_notebook(lemmatized_test_data, total=153164): \n",
    "    processed_test_data.append(remove_stopwords(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2c341ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Building stage\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d4b0854c",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features=100000      \n",
    "maxpadlen = 200          \n",
    "val_split = 0.2      \n",
    "embedding_dim_fasttext = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "72ca0cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(processed_train_data))\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(processed_train_data)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(processed_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "685f67c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words in Vocabulary:  149053\n"
     ]
    }
   ],
   "source": [
    "word_index=tokenizer.word_index\n",
    "print(\"Words in Vocabulary: \",len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7d104640",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_t=pad_sequences(list_tokenized_train, maxlen=maxpadlen, padding = 'post')\n",
    "X_te=pad_sequences(list_tokenized_test, maxlen=maxpadlen, padding = 'post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5f1247c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized sentences: \n",
      " [  116   577    11 32202   239   192    11 32202   239    90    11   579\n",
      "    11   116   366   577     1  1082   116   338  5355   116   119    11\n",
      "   387   269   366   577    11     1  1341   116    11   387     2    32\n",
      "   116   577   192   116   172    46    84   579   116   577    11     2\n",
      "   488   105    10  1087   403  1073    11  2401   489    36   116    11\n",
      "   192   407   366     9   255   192   242   150   109    18    19    29\n",
      "   172    77    19     3   257  4540     4    11 32202   239   192    11\n",
      " 32202   239    90   171   387   311    68   579    34    44    77   424\n",
      "  1550    34   171     8   579  1268    77  1638   134    10   134  4670\n",
      "   134    94    44   565  1254    17    77  1338   118   134  1577    77\n",
      "  1134     4   507    77   434    31   171    68    34   168  2207   449\n",
      "    31   315  1529    70   172   236   150   116    31    31    11    77\n",
      "    31   116    11    77    31    77    31   192   171    46    84   579\n",
      "     4    31   171   192    80    10   862  3125    11     9   255    31\n",
      "   242   150   109    18    11    77   124    67   434   116    11     9\n",
      "   357    19    29   172    77    19     3   257     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0]\n",
      "One hot label: \n",
      " [0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print('Tokenized sentences: \\n', X_t[10])\n",
    "print('One hot label: \\n', y[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d100b6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.arange(X_t.shape[0])\n",
    "np.random.shuffle(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "633d63d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_t = X_t[indices]\n",
    "labels = y[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d3bdc53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_validation_samples = int(val_split*X_t.shape[0])\n",
    "x_train = X_t[: -num_validation_samples]\n",
    "y_train = labels[: -num_validation_samples]\n",
    "x_val = X_t[-num_validation_samples: ]\n",
    "y_val = labels[-num_validation_samples: ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "af6f2e00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries in each category:\n",
      "training:  [12180  1258  6700   389  6251  1129]\n",
      "validation:  [3114  337 1749   89 1626  276]\n"
     ]
    }
   ],
   "source": [
    "print('Number of entries in each category:')\n",
    "print('training: ', y_train.sum(axis=0))\n",
    "print('validation: ', y_val.sum(axis=0)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a5fdd6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index_fasttext = {}\n",
    "f = open('wiki-news-300d-1M.vec', encoding='utf8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    embeddings_index_fasttext[word] = np.asarray(values[1:], dtype='float32')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1cdc145b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Completed!\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix_fasttext = np.random.random((len(word_index) + 1, embedding_dim_fasttext))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index_fasttext.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix_fasttext[i] = embedding_vector\n",
    "print(\" Completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3b08fb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def toxic_classifier(x_train,y_train,x_val,y_val,params):\n",
    "\n",
    "  inp=Input(shape=(maxpadlen, ),dtype='int32')\n",
    "\n",
    "  embedding_layer = Embedding(len(word_index) + 1,\n",
    "                           embedding_dim_fasttext,\n",
    "                           weights = [embedding_matrix_fasttext],\n",
    "                           input_length = maxpadlen,\n",
    "                           trainable=False,\n",
    "                           name = 'embeddings')\n",
    "  embedded_sequences = embedding_layer(inp)\n",
    "\n",
    "  x = LSTM(params['output_count_lstm'], return_sequences=True,name='lstm_layer')(embedded_sequences)\n",
    "  \n",
    "  x = GlobalMaxPool1D()(x)\n",
    "  \n",
    "  x = Dropout(params['dropout'])(x)\n",
    "  \n",
    "  x = Dense(params['output_count_dense'], activation=params['activation'], kernel_initializer='he_uniform')(x)\n",
    "  \n",
    "  x = Dropout(params['dropout'])(x)\n",
    "  \n",
    "  preds = Dense(6, activation=params['last_activation'], kernel_initializer='glorot_uniform')(x)\n",
    "\n",
    "  model = Model(inputs=inp, outputs=preds)\n",
    "\n",
    "  model.compile(loss=params['loss'], optimizer=params['optimizer'], metrics=['accuracy'])\n",
    "\n",
    "  model_info=model.fit(x_train,y_train, epochs=params['epochs'], batch_size=params['batch_size'],  validation_data=(x_val, y_val))\n",
    "\n",
    "  return model_info, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9c06a1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "p={\n",
    "    'output_count_lstm': [40,50,60],\n",
    "    'output_count_dense': [30,40,50],\n",
    "    'batch_size': [32],\n",
    "    'epochs':[2],\n",
    "    'optimizer':['adam'],\n",
    "    'activation':['relu'],\n",
    "    'last_activation': ['sigmoid'],\n",
    "    'dropout':[0.1,0.2],\n",
    "    'loss': ['binary_crossentropy']   \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "40577c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def toxic_classifier(x_train,y_train,x_val,y_val,params):\n",
    "\n",
    "  inp=Input(shape=(maxpadlen, ),dtype='int32')\n",
    "\n",
    "  embedding_layer = Embedding(len(word_index) + 1,\n",
    "                           embedding_dim_fasttext,\n",
    "                           weights = [embedding_matrix_fasttext],\n",
    "                           input_length = maxpadlen,\n",
    "                           trainable=False,\n",
    "                           name = 'embeddings')\n",
    "  embedded_sequences = embedding_layer(inp)\n",
    "\n",
    "  x = LSTM(params['output_count_lstm'], return_sequences=True,name='lstm_layer')(embedded_sequences)\n",
    "\n",
    "  x = Conv1D(filters=params['filters'], kernel_size=params['kernel_size'], padding='same', activation='relu', kernel_initializer='he_uniform')(x)\n",
    "\n",
    "  x = MaxPooling1D(params['pool_size'])(x)\n",
    "  \n",
    "  x = GlobalMaxPool1D()(x)\n",
    "  \n",
    "  x = BatchNormalization()(x)\n",
    "  \n",
    "  x = Dense(params['output_1_count_dense'], activation=params['activation'], kernel_initializer='he_uniform')(x)\n",
    "  \n",
    "  x = Dropout(params['dropout'])(x)\n",
    "\n",
    "  x = Dense(params['output_2_count_dense'], activation=params['activation'], kernel_initializer='he_uniform')(x)\n",
    "  \n",
    "  x = Dropout(params['dropout'])(x)\n",
    "  \n",
    "  preds = Dense(6, activation=params['last_activation'], kernel_initializer='glorot_uniform')(x)\n",
    "\n",
    "  model = Model(inputs=inp, outputs=preds)\n",
    "\n",
    "  model.compile(loss=params['loss'], optimizer=params['optimizer'], metrics=['accuracy'])\n",
    "\n",
    "  model_info=model.fit(x_train,y_train, epochs=params['epochs'], batch_size=params['batch_size'],  validation_data=(x_val, y_val))\n",
    "\n",
    "  return model_info, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0d981882",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp=Input(shape=(maxpadlen, ),dtype='int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7c1600e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                           embedding_dim_fasttext,\n",
    "                           weights = [embedding_matrix_fasttext],\n",
    "                           input_length = maxpadlen,\n",
    "                           trainable=False,\n",
    "                           name = 'embeddings')\n",
    "embedded_sequences = embedding_layer(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a335ee38",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = LSTM(40, return_sequences=True,name='lstm_layer')(embedded_sequences)\n",
    "x = GlobalMaxPool1D()(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(30, activation=\"relu\", kernel_initializer='he_uniform')(x)\n",
    "x = Dropout(0.1)(x)\n",
    "preds = Dense(6, activation=\"sigmoid\", kernel_initializer='glorot_uniform')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3f644b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_1 = Model(inputs=inp, outputs=preds)\n",
    "model_1.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ed5ffd7d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 200)]             0         \n",
      "                                                                 \n",
      " embeddings (Embedding)      (None, 200, 300)          44716200  \n",
      "                                                                 \n",
      " lstm_layer (LSTM)           (None, 200, 40)           54560     \n",
      "                                                                 \n",
      " global_max_pooling1d (Globa  (None, 40)               0         \n",
      " lMaxPooling1D)                                                  \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 40)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 30)                1230      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 30)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 6)                 186       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 44,772,176\n",
      "Trainable params: 55,976\n",
      "Non-trainable params: 44,716,200\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#NO NEED TO RUN\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e1f1190e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = keras.models.load_model(filepath=\"/Models/LSTM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "22561055",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_values_1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [62]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m sample_submission \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msample_submission.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m test_values_1\u001b[38;5;241m=\u001b[39mpd\u001b[38;5;241m.\u001b[39mDataFrame(\u001b[43mtest_values_1\u001b[49m,columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoxic\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msevere_toxic\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobscene\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthreat\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minsult\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124midentity_hate\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      3\u001b[0m submission \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(sample_submission[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m      4\u001b[0m combined_submission\u001b[38;5;241m=\u001b[39mpd\u001b[38;5;241m.\u001b[39mconcat([submission,test_values_1],axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test_values_1' is not defined"
     ]
    }
   ],
   "source": [
    "#NO NEED TO RUN\n",
    "sample_submission = pd.read_csv('sample_submission.csv')\n",
    "test_values_1=pd.DataFrame(test_values_1,columns=['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'])\n",
    "submission = pd.DataFrame(sample_submission[\"id\"])\n",
    "combined_submission=pd.concat([submission,test_values_1],axis=1)\n",
    "combined_submission.to_csv('File_Path', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d4d94559",
   "metadata": {},
   "outputs": [],
   "source": [
    "def toxicity_level(string):\n",
    "    new_string = [string]\n",
    "    new_string = tokenizer.texts_to_sequences(new_string)\n",
    "    new_string = pad_sequences(new_string, maxlen=maxpadlen, padding='post')\n",
    "    \n",
    "    prediction = model_1.predict(new_string) #(Change to model_1 or model_2 depending on the preference of model type|| Model 1: LSTM, Model 2:LSTM-CNN)\n",
    "    \n",
    "    print(\"Toxicity levels for '{}':\".format(string))\n",
    "    print('Toxic:         {:.0%}'.format(prediction[0][0]))\n",
    "    print('Severe Toxic:  {:.0%}'.format(prediction[0][1]))\n",
    "    print('Obscene:       {:.0%}'.format(prediction[0][2]))\n",
    "    print('Threat:        {:.0%}'.format(prediction[0][3]))\n",
    "    print('Insult:        {:.0%}'.format(prediction[0][4]))\n",
    "    print('Identity Hate: {:.0%}'.format(prediction[0][5]))\n",
    "    print()\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9784255c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 324ms/step\n",
      "Toxicity levels for 'Test':\n",
      "Toxic:         3%\n",
      "Severe Toxic:  0%\n",
      "Obscene:       1%\n",
      "Threat:        0%\n",
      "Insult:        1%\n",
      "Identity Hate: 0%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "toxicity_level('Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5963dcd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 28ms/step\n",
      "Toxicity levels for 'American guy':\n",
      "Toxic:         18%\n",
      "Severe Toxic:  0%\n",
      "Obscene:       2%\n",
      "Threat:        2%\n",
      "Insult:        5%\n",
      "Identity Hate: 3%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "toxicity_level('American guy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c082526",
   "metadata": {},
   "source": [
    "# Twitter Integration\n",
    "# Authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "004c191d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successful Authentication\n"
     ]
    }
   ],
   "source": [
    "import tweepy\n",
    "import pandas as pd\n",
    " \n",
    "# API keys that yous saved earlier\n",
    "api_key = \"enuBnRMH8729m3W0Wwk1EPyCF\"\n",
    "api_secrets = \"PY2hOyypLYpMDnDEViInru6LgoVl2WkcdzSdVdMDVjqZ6VrlBV\"\n",
    "access_token = \"3066529674-1bbgakeXcsSlm0XOvc6mH2F3JKNX3QJD58C18S3\"\n",
    "access_secret = \"CjJk8gtBA2C4way26bHBRk1YfxldAf0wBoqWjgnyOfyCa\"\n",
    " \n",
    "# Authenticate to Twitter\n",
    "auth = tweepy.OAuthHandler(api_key,api_secrets)\n",
    "auth.set_access_token(access_token,access_secret)\n",
    " \n",
    "api = tweepy.API(auth)\n",
    " \n",
    "try:\n",
    "    api.verify_credentials()\n",
    "    print('Successful Authentication')\n",
    "except:\n",
    "    print('Failed authentication')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a5137d",
   "metadata": {},
   "source": [
    "## Save username of the user to a variable to obtain the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "875658dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "user = api.get_user(screen_name='AhmedLPG')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "93eac1c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199\n"
     ]
    }
   ],
   "source": [
    "# Print number of tweets\n",
    "print(len(tweets))\n",
    "#200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "65443d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = api.user_timeline(screen_name='AhmedLPG', count=200)\n",
    "tweets_extended = api.user_timeline(screen_name='AhmedLPG', tweet_mode='extended', count=200)\n",
    "to_extract = [\n",
    "    'id',\n",
    "    'created_at',\n",
    "    'text',\n",
    "    'full_text',\n",
    "    'lang',\n",
    "    'retweeted'\n",
    "    ]\n",
    " \n",
    "tweet_entities = [\n",
    "    ('hashtags','text')\n",
    "]\n",
    "tweets_data = []\n",
    " \n",
    "for tweet in tweets_extended:\n",
    "    tweet = tweet._json\n",
    "    tweet_data = {}\n",
    "    for e in to_extract:\n",
    "        try:\n",
    "            tweet_data[e]=tweet[e]\n",
    "        except:\n",
    "            continue\n",
    "    for entity in tweet_entities:\n",
    "        entity_list = []\n",
    "        for t in tweet['entities'][entity[0]]:\n",
    "            entity_list.append(t[entity[1]])\n",
    "        tweet_data[entity[0]] = entity_list\n",
    " \n",
    "    tweets_data.append(tweet_data)\n",
    "    # Get Tweet data\n",
    "df = pd.DataFrame(tweets_data)\n",
    "EngTweets = df[(df['lang']=='en') & (df['retweeted'] == False)]\n",
    "EngTweetsFirstRow =EngTweets['full_text'].iloc[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "069e5d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He is a retard\n"
     ]
    }
   ],
   "source": [
    "print(EngTweetsFirstRow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b16af2da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 53ms/step\n",
      "Toxicity levels for 'He is a retard':\n",
      "Toxic:         81%\n",
      "Severe Toxic:  3%\n",
      "Obscene:       38%\n",
      "Threat:        3%\n",
      "Insult:        53%\n",
      "Identity Hate: 3%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "toxicity_level(EngTweetsFirstRow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec27d9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
